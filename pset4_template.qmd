---
title: "PS4 Nasser & Daniel"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (Nasser Alshaya and alshaya):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\NA\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. I pulled the columns related to: Provider Subtype, Provider Type, Number of Times of Changing Ownership, Effective Date of Most Recent Ownership Change, City Name, Facility Name, CMS Certification Number, Termination Status, Date the provider was Terminated, and Zip Code.

```{python}
import pandas as pd
import os
base_path = r"/Users/nasser.alshaya/Desktop/problem-set-4-nasser-daniel/POS"
path_data = os.path.join(base_path,"pos2016.csv")
df = pd.read_csv(path_data)
df.columns
```

2. 
    a. There are `7245` hosptials reported in this data as short-term hospitals in `2016` Q4. The number is too large to be true, we would assume that most of the hospitals of this type are no longer in service.
    b. Accordin to **[2016 CMS Statistics](https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/CMS-Statistics-Reference-Booklet/Downloads/2016_CMS_Stats.pdf)**, the number of short-term hospitals as of December 2015 is `3436`, the number is way smaller than what we found in (a), even if the difference is only one year. 

```{python}
df_2016 = df[
    (df["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df["PRVDR_CTGRY_CD"] == 1)]
len(df_2016)
```

3. The number of short-term hospitals per is plotted below, the number of hospitals increases every year, starting from 7245 in 2016 reaching to 7303 in 2019. 
```{python}
path_data = os.path.join(base_path,
 "pos2017.csv")
df_2017 = pd.read_csv(path_data)
 
path_data = os.path.join(base_path,
 "pos2018.csv")
df_2018 = pd.read_csv(path_data, encoding_errors = "ignore")

path_data = os.path.join(base_path,
 "pos2019.csv")
df_2019 = pd.read_csv(path_data, encoding_errors = "ignore")
```


```{python}
df_2017 = df_2017[
    (df_2017["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2017["PRVDR_CTGRY_CD"] == 1)]

df_2018 = df_2018[
    (df_2018["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2018["PRVDR_CTGRY_CD"] == 1)]

df_2019 = df_2019[
    (df_2019["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2019["PRVDR_CTGRY_CD"] == 1)]

# Add year column for each df for plot:
df_2016["year"] = 2016
df_2017["year"] = 2017
df_2018["year"] = 2018
df_2019["year"] = 2019
df_pos = pd.concat([df_2016,df_2017,df_2018,df_2019],
 ignore_index = True)
```

```{python}
import altair as alt
import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
alt.data_transformers.disable_max_rows()
bars = alt.Chart(df_pos).mark_bar().encode(
    alt.X("year:N", title = "Year"),
    alt.Y("count()",title = None, axis = None)
).properties(
    title = "Number of Short-term Hospitals per Year",
    width = 350, height = 200)
text = bars.mark_text(
    align="center",
    baseline="bottom",
    dy=-5  
).encode(
    text=alt.Text("count():Q")  
)

bars + text
```

4. 
    a. Upon aggregating the data based on the CMS certificate of each hospital, it seems that the numbers are identical to what previously found in (3)

```{python}
bars = alt.Chart(df_pos).transform_aggregate(
    unique_hospitals="count(PRVDR_NUM)",groupby=["year"]
    ).mark_bar().encode(
        x=alt.X("year:N", title="Year"),
    y=alt.Y("unique_hospitals:Q", axis = None, title = None)
).properties(
    title="Count of Unique Hospitals per Year",
    width=350,
    height=200
)
text = bars.mark_text(
    align="center",
    baseline="bottom",
    dy=-5  
).encode(
    text=alt.Text("unique_hospitals:Q")  
)

bars + text
```

    b. Upon comparing the two plots, the number of hospitals per year is identical to the number of unique hospitals based on the CMS certificate, this tells us that the data is strucutured in a way that prevents duplicate entries of hospitals per year, and the records are well strucutured to represent the number of hospitals accurately.

  

## Identify hospital closures in POS file (15 pts) (*)

1. 
2. 
3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 

1. 
    a. There are five files zipped in this folder as the following:

    `dbf`: stores attribute information of spatial features, size 6.4 MB.
    `shp`: shape file which has has feature geometrics, size 837.5 MB.
    `prj`: describes the Coordinate Reference System (CRS), size 165 bytes.
    `shx`: contains positional index in .shp, size 265 KB.
    `xml`: metadata about the dataset, size 16 KB.

    b.     `dbf`:size 6.4 MB.
    `shp`: size 837.5 MB.
    `prj`: size 165 bytes.
    `shx`: size 265 KB.
    `xml`: size 16 KB.
    
2. 
```{python}
import geopandas as gpd 
```

```{python}
filepath = "/Users/nasser.alshaya/Desktop/problem-set-4-nasser-daniel/gz_2010_us_860_00_500k"

path_shp = os.path.join(filepath,
 "gz_2010_us_860_00_500k.shp")
df_zip = gpd.read_file(path_shp)
```

```{python}
texas_zip_codes = df_zip[df_zip["ZCTA5"].str.startswith(
    ("70","71","72","73","74","75","76","77","78","79"))]
```

```{python}
'''
texas_zip_codes[texas_zip_codes["ZCTA5"].isin(df_2016["ZIP_CD"])]
'''
```

```{python}
import matplotlib.pyplot as plt
texas_zip_codes.plot(facecolor="none", linewidth=0.2)
plt.axis("off")
plt.show()
```


## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
