---
title: "Problem Set 4: Spatial"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts)

(Note: Used ChatGPT to troubleshoot why pd.read_csv wasn't working- learned to specify a different encoding other than utf-8)


1. Which variables do we need to complete the exercise:
- Short-term hospitals
  -Certification Date (CRTFCTN_DT)
  -Change of ownership date (CHOW_DT)
  -Provider code 1 (PRVDR_CTGRY_CD)
  -subtype 1 (PRVDR_CTGRY_SBTYP_CD)
  -CMS Certification number (PRVDR_NUM)
  -Termination Code (PGM_TRMNTN_CD)
  -Facility Name (FAC_NAME)
  -Facility Zip (ZIP_CD)

2. 
    a. Import the pos2016.csv file and subset it:
```{python}
import pandas as pd
df = pd.read_csv('pos_2016.csv', encoding='Windows-1252', low_memory=False)

df.columns = ["prvdr_subtype_code", "prvdr_category_code", "change_of_owner_date", "certification_date", "facility_name", "prvdr_num", "termination_code", "zip_code"]

#The date fields are numbers, so we need to change the numbers in the date fields to strings and then date objects. Attribution: Used ChatGPT to assist with regex issues
df['change_of_owner_date'] = pd.to_datetime(df['change_of_owner_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

df['certification_date'] = pd.to_datetime(df['certification_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

df = df[(df["prvdr_category_code"] == 1) & (df["prvdr_subtype_code"] == 1)]

```

  b. After subsetting to only short-term hospitals with these codes, we find 7245 hospitals in our data set. The provided KFF article cites a number of "nearly 5000".

This discrepancy could be due to potential duplicates in our data (if a hospital was acquired or merged into a newer hospital, it may be counted twice). Additionally, our data may include hospitals that are short-term hospitals, but not specifically acute-care hospitals providing emergency care/surgery and other interventions, which the KFF article might exclude.

3. Repeat the previous steps with 2017Q4, 2018Q4, and 2019Q4 and then append them together: 

```{python}

def read_clean_filter(pathname):
  """Reads the raw csv files from CMS, renames the columns, and filters to only short term hospitals.
  Also updates the date columns to pd.datetime format
  """
  df = pd.read_csv(pathname, encoding='Windows-1252', low_memory=False)

  df.columns = ["prvdr_subtype_code", "prvdr_category_code", "change_of_owner_date", "certification_date", "facility_name", "prvdr_num", "termination_code", "zip_code"]
  
  df['change_of_owner_date'] = pd.to_datetime(df['change_of_owner_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

  df['certification_date'] = pd.to_datetime(df['certification_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

  df = df[(df["prvdr_category_code"] == 1) & (df["prvdr_subtype_code"] == 1)]

  return df

df_2017 = read_clean_filter("pos_2017.csv")
df_2018 = read_clean_filter("pos_2018.csv")
df_2019 = read_clean_filter("pos_2019.csv")

#Merge all dataframes from 2016-2019 together
#Use this for future pset problems

df_all = pd.concat([df, df_2017, df_2018, df_2019], ignore_index=True)

```

Now we plot the # of observations in the dataset by year of certification date:

```{python}

observations_by_year = df_all.alt.Chart().mark_point.encode(
  x = alt.X('year')
)
# The above doesn't run
```

4. 
    a.
    b.

## Identify hospital closures in POS file (15 pts) (*)

```{python}

import geopandas
import numpy as np

```

1. Create a list of hospitals active in 2016, closed by 2019 (facility name, zip, year of closure). How many are there?

```{python}

# Create temporary, smaller dataframes for this purpose
df_2016_temp = df[['facility_name', 'prvdr_num',
                   'zip_code', 'termination_code']]
df_2017_temp = df_2017[['facility_name',
                        'prvdr_num', 'zip_code', 'termination_code']]
df_2018_temp = df_2018[['facility_name',
                        'prvdr_num', 'zip_code', 'termination_code']]
df_2019_temp = df_2019[['facility_name',
                        'prvdr_num', 'zip_code', 'termination_code']]

# Merge them on provider number to have all the term codes together by year
df_terms = df_2016_temp.merge(df_2017_temp, on=['prvdr_num'],
                              how='outer', suffixes=('', '_2017')
                              ).merge(df_2018_temp, on=['prvdr_num'],
                                      how='outer', suffixes=('', '_2018')
                                      ).merge(df_2019_temp, on=['prvdr_num'],
                                              how='outer', suffixes=('', '_2019'))

# Rename 2016 column & filter for only those active in 2016
df_terms = df_terms.rename(
    columns={'termination_code': 'termination_code_2016', 
    'zip_code': 'zip_code_2016'})
df_terms = df_terms[df_terms['termination_code_2016'] == 0]

# Filter to find only those who closed throughout the years
df_16_19_closures = df_terms[
    (df_terms['termination_code_2017'] != 0) |
    (df_terms['termination_code_2018'] != 0) |
    (df_terms['termination_code_2019'] != 0)]

# Function to find closure year, or nan if not closed
def closure_year(row):
    for year in range(2017, 2020):
        termination_code = row[f'termination_code_{year}']
        if termination_code != 0:
            return year
    return np.nan

# Apply the function (found .apply() on stackoverflow
# https://stackoverflow.com/questions/19914937/applying-function-
# with-multiple-arguments-to-create-a-new-pandas-column)
df_16_19_closures['closure_year'] = df_16_19_closures.apply(
    closure_year, axis=1)

# Find how many
print(len(df_16_19_closures))

```

There are 174 hospitals that fit this definition, but it's worth noting that some of the hospitals close and then reopen. Some of the reopenings are under different names, but with the same provider code. 

2. Sort by name and report first 10.

```{python}

df_2_2 = df_16_19_closures[['facility_name', 'prvdr_num',
                            'termination_code_2016',
                            'termination_code_2017',
                            'termination_code_2018',
                            'termination_code_2019']]
                            
print(df_2_2.sort_values(by='facility_name').head(10))

```

3. Remove closures in ZIPs where number of closures doesnt decrease

```{python}



```

    a.
    b.

## Download Census zip code shapefile (10 pt) 

1. 
    a.
    b. 
2. 

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
    c.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
