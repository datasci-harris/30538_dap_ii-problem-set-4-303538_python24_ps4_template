---
title: "Problem Set 4: Spatial"
title: "Problem Set 4: Spatial"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts)

(Note: Used ChatGPT to troubleshoot why pd.read_csv wasn't working- learned to specify a different encoding other than utf-8)


1. Which variables do we need to complete the exercise:
- Short-term hospitals
  -Certification Date (CRTFCTN_DT)
  -Change of ownership date (CHOW_DT)
  -Provider code 1 (PRVDR_CTGRY_CD)
  -subtype 1 (PRVDR_CTGRY_SBTYP_CD)
  -CMS Certification number (PRVDR_NUM)
  -Termination Code (PGM_TRMNTN_CD)
  -Facility Name (FAC_NAME)
  -Facility Zip (ZIP_CD)

2. 
    a. Import the pos2016.csv file and subset it:


```{python}
import pandas as pd
df = pd.read_csv('pos_2016.csv', encoding='Windows-1252', low_memory=False)
```

```{python}
import pandas as pd
df = pd.read_csv('pos_2016.csv', encoding='Windows-1252', low_memory=False)

df.columns = ["prvdr_subtype_code", "prvdr_category_code", "change_of_owner_date", "certification_date", "facility_name", "prvdr_num", "zip_code", "termination_code", "termination_date"]

#The date fields are numbers, so we need to change the numbers in the date fields to strings and then date objects. Attribution: Used ChatGPT to assist with regex issues
df['change_of_owner_date'] = pd.to_datetime(df['change_of_owner_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

df['certification_date'] = pd.to_datetime(df['certification_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

df['termination_date'] = pd.to_datetime(df['termination_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

df = df[(df["prvdr_category_code"] == 1) & (df["prvdr_subtype_code"] == 1)]
```

  b. After subsetting to only short-term hospitals with these codes, we find 7245 hospitals in our data set. The provided KFF article cites a number of "nearly 5000".

This discrepancy could be due to potential duplicates in our data (if a hospital was acquired or merged into a newer hospital, it may be counted twice). Additionally, our data may include hospitals that are short-term hospitals, but not specifically acute-care hospitals providing emergency care/surgery and other interventions, which the KFF article might exclude.

3. Repeat the previous steps with 2017Q4, 2018Q4, and 2019Q4 and then append them together: 

```{python}

def read_clean_filter(pathname):
  """Reads the raw csv files from CMS, renames the columns, and filters to only short term hospitals.
  Also updates the date columns to pd.datetime format
  """
  df = pd.read_csv(pathname, encoding='Windows-1252', low_memory=False)

  df.columns = ["prvdr_subtype_code", "prvdr_category_code", "change_of_owner_date", "certification_date", "facility_name", "prvdr_num", "zip_code", "termination_code", "termination_date"]
  
  df['change_of_owner_date'] = pd.to_datetime(df['change_of_owner_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

  df['certification_date'] = pd.to_datetime(df['certification_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

  df['termination_date'] = pd.to_datetime(df['termination_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

  df = df[(df["prvdr_category_code"] == 1) & (df["prvdr_subtype_code"] == 1)]

  return df

df_2017 = read_clean_filter("pos_2017.csv")
df_2018 = read_clean_filter("pos_2018.csv")
df_2019 = read_clean_filter("pos_2019.csv")

#Merge all dataframes from 2016-2019 together
#Use this for future pset problems

df_all = pd.concat([df, df_2017, df_2018, df_2019], ignore_index=True)

```

Now we plot the # of observations in the dataset by year of certification date:

```{python}

import altair as alt

df_all['year'] = df_all['certification_date'].dt.year
summary = df_all.groupby('year').size().reset_index(name='count')

observations_by_year = alt.Chart(summary).mark_bar().encode(
  x = alt.X('year:O', title="Year"),
  y = alt.Y('count', title="Number of Observations")
)

observations_by_year

```

4. 
a. Now we repeat the previous step but this time plot unique hospitals by year instead of just observations:

```{python}

unique_hospitals = df_all.drop_duplicates(subset=['prvdr_num', 'year'])
unique_summary = unique_hospitals.groupby('year').size().reset_index(name='count')

unique_hospitals_by_year = alt.Chart(unique_summary).mark_bar().encode(
  x = alt.X('year:O', title="Year"),
  y = alt.Y('count', title="Number of Unique Hospitals")
)

unique_hospitals_by_year

```


b. Not only are the plots different, we can see two things: 

1. The maximum number of hospitals is much smaller than the maximum number of observations, implying there are a lot of repeat rows in the data for the same facilities.
2. The graph of unique hospitals is much more skewed to the right than simply graphing observations, implying that either many new hospitals were certified from 2014 onwards, or that many old hospitals were merged/rebranded into new hospitals from 2014 onwards. Based on what we've read in the KFF article, it's highly likely that recent corporate/economic pressures caused hospitals to merge/rebrand leading to a rush of new certifications, rather than an actual increase in the amount of hospital capacity.

## Identify hospital closures in POS file (15 pts) (*)

```{python}

import geopandas
import numpy as np

```

1. Create a list of hospitals active in 2016, closed by 2019 (facility name, zip, year of closure). How many are there?

```{python}

# Create temporary, smaller dataframes for this purpose
df_2016_temp = df[['facility_name', 'prvdr_num',
                   'zip_code', 'termination_code']]
df_2017_temp = df_2017[['facility_name',
                        'prvdr_num', 'zip_code', 'termination_code']]
df_2018_temp = df_2018[['facility_name',
                        'prvdr_num', 'zip_code', 'termination_code']]
df_2019_temp = df_2019[['facility_name',
                        'prvdr_num', 'zip_code', 'termination_code']]

# Merge them on provider number to have all the term codes together by year
df_terms = df_2016_temp.merge(df_2017_temp, on=['prvdr_num'],
                              how='outer', suffixes=('', '_2017')
                              ).merge(df_2018_temp, on=['prvdr_num'],
                                      how='outer', suffixes=('', '_2018')
                                      ).merge(df_2019_temp, on=['prvdr_num'],
                                              how='outer', suffixes=('', '_2019'))

# Rename 2016 column & filter for only those active in 2016
df_terms = df_terms.rename(
    columns={'termination_code': 'termination_code_2016', 
    'zip_code': 'zip_code_2016'})
df_terms = df_terms[df_terms['termination_code_2016'] == 0]

# Filter to find only those who closed sometime throughout the years
df_16_19_closures = df_terms[
    (df_terms['termination_code_2017'] != 0) |
    (df_terms['termination_code_2018'] != 0) |
    (df_terms['termination_code_2019'] != 0)]

# Function to find closure year for each hospital
def closure_year(row):
    for year in range(2017, 2020):
        termination_code = row[f'termination_code_{year}']
        if termination_code != 0:
            return year

# Apply the function (found .apply() on stackoverflow
# https://stackoverflow.com/questions/19914937/applying-function-
# with-multiple-arguments-to-create-a-new-pandas-column)
df_16_19_closures['closure_year'] = df_16_19_closures.apply(
    closure_year, axis=1)

# Find how many
print(len(df_16_19_closures))

```

There are 174 hospitals that fit this definition, but it's worth noting that some of the hospitals close and then reopen. Some of the reopenings are under different names but with the same provider code, and some are enitrely different. 

2. Sort by name and report first 10.

```{python}

df_2_2 = df_16_19_closures[['facility_name', 
                            'closure_year']]
                            
print(df_2_2.sort_values(by='facility_name').head(10))

```

3. Remove closures in ZIPs where number of active hospitals doesnt decrease

```{python}

active_counts_by_zip = {}

# Loop through each year and count active hospitals by ZIP code
for year in range(2016, 2020):
    active_hospitals = df_16_19_closures[df_16_19_closures[f'termination_code_{
        year}'] == 0]
    active_counts = active_hospitals.groupby(
        'zip_code_2016').size().reset_index(name=f'active_hospitals_{year}')
    active_counts_by_zip[year] = active_counts

# Merge active hospital counts on zip code, missing implies closed, so 0 count
active_hospitals_by_year = active_counts_by_zip[2016]
for year in range(2017, 2020):
    active_hospitals_by_year = active_hospitals_by_year.merge(
        active_counts_by_zip[year], on='zip_code_2016', how='outer')
active_hospitals_by_year = active_hospitals_by_year.fillna(0)

# We can't use 2016 or 2020, so find 17-18 and 18-19
active_hospitals_by_year['change_2017_2018'] = active_hospitals_by_year[
    'active_hospitals_2018'] - active_hospitals_by_year['active_hospitals_2017']
active_hospitals_by_year['change_2018_2019'] = active_hospitals_by_year[
    'active_hospitals_2019'] - active_hospitals_by_year['active_hospitals_2018']

# This contains only hospitals that 'closed' in 17 or 18
df_17_closures = df_16_19_closures[df_16_19_closures['closure_year'] == 2017]
df_18_closures = df_16_19_closures[df_16_19_closures['closure_year'] == 2018]

df_change_17_18 = active_hospitals_by_year[active_hospitals_by_year['change_2017_2018'] == 0]
df_change_18_19 = active_hospitals_by_year[active_hospitals_by_year['change_2018_2019'] == 0]

df_17_no_change = df_17_closures[~df_17_closures['zip_code_2016'].isin(df_change_17_18['zip_code_2016'])]
df_18_no_change = df_18_closures[~df_18_closures['zip_code_2016'].isin(df_change_18_19['zip_code_2016'])]

```

    a.
    b.

## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types are:
      1. .shp - The file that contains the actual geoms (points, lines, polygons) that will be displayed
      2. .dbf - Stores attribute data for each shape in the .shp file, like population, income, temperature, etc. 
      3. .shx - An index file that points to specific shapes in the .shp file for easier navigation
      4. .prj - A projection file that stores the coordinate information for latitude/longitude 
      5. .xml - Metadata file that is mostly used for documentation/readme purposes, including data sources, attributes, etc.
    b. The .shp file is the largest at 837.5 MB, while the other files are:
      .dbf - 6.4 MB
      .shx - 0.26 MB
      .xml - 16 KB
      .prj - 0.16 KB
2. To open the shapefile, we will install and use geopandas:

```{python}
import geopandas as gpd
zip_codes = gpd.read_file("/Users/charleshuang/Downloads/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp")

#Texas zip codes always start with 75, 76, or 77, and ZCTA5 is the column with the zip codes
#Used ChatGPT to find a function that can identify strings by the first few characters
tx_zip_codes = zip_codes[zip_codes["ZCTA5"].str.startswith(('75', '76', '77'))]

print(tx_zip_codes.head())
```

Now we calculate the # of hospitals per zip code in 2016 based on the previous step:

```{python}

import shapely
from shapely import Polygon, Point

#We need to merge our previous hospital data with the shapefile zip_codes data:

```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
    c.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
