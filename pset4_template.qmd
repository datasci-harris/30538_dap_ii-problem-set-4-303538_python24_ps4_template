---
title: "PS4 Nasser & Daniel"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (Nasser Alshaya and alshaya):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\NA\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. I pulled the columns related to: Provider Subtype, Provider Type, Number of Times of Changing Ownership, Effective Date of Most Recent Ownership Change, City Name, Facility Name, CMS Certification Number, Termination Status, Date the provider was Terminated, and Zip Code.

```{python}
import pandas as pd
import os
<<<<<<< HEAD
base_path = r"C:/Users/danie/Documents/GitHub/problem-set-4-nasser-daniel/POS"
path_data = os.path.join(base_path,
 "pos2016.csv")
=======
base_path = r"/Users/nasser.alshaya/Desktop/problem-set-4-nasser-daniel/POS"
path_data = os.path.join(base_path,"pos2016.csv")
>>>>>>> e18afaa8fa88b06ed138d7305aab9fab47b152b1
df = pd.read_csv(path_data)
df.columns
```
```{python}
df_2016 = df[
    (df["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df["PRVDR_CTGRY_CD"] == 1)]
len(df_2016)
```

3. The number of short-term hospitals per is plotted below, the number of hospitals increases every year, starting from 7245 in 2016 reaching to 7303 in 2019. 
```{python}
path_data = os.path.join(base_path,
 "pos2017.csv")
df_2017 = pd.read_csv(path_data)
 
path_data = os.path.join(base_path,
 "pos2018.csv")
df_2018 = pd.read_csv(path_data, encoding_errors = "ignore")

path_data = os.path.join(base_path,
 "pos2019.csv")
df_2019 = pd.read_csv(path_data, encoding_errors = "ignore")
```


```{python}
df_2017 = df_2017[
    (df_2017["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2017["PRVDR_CTGRY_CD"] == 1)]

df_2018 = df_2018[
    (df_2018["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2018["PRVDR_CTGRY_CD"] == 1)]

df_2019 = df_2019[
    (df_2019["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2019["PRVDR_CTGRY_CD"] == 1)]

# Add year column for each df for plot:
df_2016["year"] = 2016
df_2017["year"] = 2017
df_2018["year"] = 2018
df_2019["year"] = 2019
df_pos = pd.concat([df_2016,df_2017,df_2018,df_2019],
 ignore_index = True)
```

```{python}
import altair as alt
import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
alt.data_transformers.disable_max_rows()
bars = alt.Chart(df_pos).mark_bar().encode(
    alt.X("year:N", title = "Year"),
    alt.Y("count()",title = None, axis = None)
).properties(
    title = "Number of Short-term Hospitals per Year",
    width = 350, height = 200)
text = bars.mark_text(
    align="center",
    baseline="bottom",
    dy=-5  
).encode(
    text=alt.Text("count():Q")  
)

bars + text
```

4. 
    a. Upon aggregating the data based on the CMS certificate of each hospital, it seems that the numbers are identical to what previously found in (3)

```{python}
bars = alt.Chart(df_pos).transform_aggregate(
    unique_hospitals="count(PRVDR_NUM)",groupby=["year"]
    ).mark_bar().encode(
        x=alt.X("year:N", title="Year"),
    y=alt.Y("unique_hospitals:Q", axis = None, title = None)
).properties(
    title="Count of Unique Hospitals per Year",
    width=350,
    height=200
)
text = bars.mark_text(
    align="center",
    baseline="bottom",
    dy=-5  
).encode(
    text=alt.Text("unique_hospitals:Q")  
)

bars + text
```

    b. Upon comparing the two plots, the number of hospitals per year is identical to the number of unique hospitals based on the CMS certificate, this tells us that the data is strucutured in a way that prevents duplicate entries of hospitals per year, and the records are well strucutured to represent the number of hospitals accurately.

  

## Identify hospital closures in POS file (15 pts) (*)

1. 
```{python}
df_pos["PRVDR_NUM"] = df_pos["PRVDR_NUM"].astype(int)
#PGM_TRMNTN_CD is the indicator variable for active/inactive status in a given year
def active_inactive(active_year, inactive_year):
    #to capture the hospitals that were active in the year prior, ex. 2016
    starting_year = df_pos[(df_pos["year"] == active_year) & (df_pos["PGM_TRMNTN_CD"] == 0)]

    #identify the codes of active hospitals in that year, ex. active hospitals in 2016
    starting_year_codes = starting_year["PRVDR_NUM"].unique()

    #identify hospitals in following year (ex. 2017) using codes of previously active hospitals
    following_year_active = df_pos[(df_pos["year"] == inactive_year) & df_pos["PRVDR_NUM"].isin(starting_year_codes)]

    #filtering to identify closures in following year (ex. hospitals that closed in 2017)
    following_year_inactive = following_year_active[following_year_active["PGM_TRMNTN_CD"] != 0]
    return following_year_inactive

#active 2016, inactive 2017
active_inactive_2016_2017 = active_inactive(2016, 2017)

#active 2017, inactive 2018
active_inactive_2017_2018 = active_inactive(2017, 2019)

#active 2018, inactive 2018
active_inactive_2018_2019 = active_inactive(2018, 2019)

inactive_hospitals = pd.concat([active_inactive_2016_2017, active_inactive_2017_2018, active_inactive_2018_2019])

#accessing the last available year for each of the inactive hospitals
fac_name_year_list = []
inactive_hospitals_unique = inactive_hospitals["PRVDR_NUM"].unique()
for i in inactive_hospitals_unique:
    hospital = inactive_hospitals[inactive_hospitals["PRVDR_NUM"] == i]
    fac_name_year = []
    fac_name_year.append(hospital["year"].max())
    fac_name_year.append(hospital["FAC_NAME"].unique()[0])
    fac_name_year_list.append(fac_name_year)
    
#turning the list into a dataframe, and then merging that with selected columns to answer the question.
fac_name_year_df = pd.DataFrame(fac_name_year_list, columns = ["year", "FAC_NAME"])
facility_name_zip = inactive_hospitals[["FAC_NAME", "ZIP_CD"]]
facility_name_zip = facility_name_zip.merge(fac_name_year_df, how = "left", on = "FAC_NAME")

facility_name_zip.shape[0]
```

There are 259 hospitals that meet this criteria.

2. 
```{python}
facility_name_zip = facility_name_zip.sort_values(by = "FAC_NAME")
facility_name_zip.head(10)[["FAC_NAME", "year"]]

```


3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 

1. 
    a. There are five files zipped in this folder as the following:

    `dbf`: stores attribute information of spatial features, size 6.4 MB.
    `shp`: shape file which has has feature geometrics, size 837.5 MB.
    `prj`: describes the Coordinate Reference System (CRS), size 165 bytes.
    `shx`: contains positional index in .shp, size 265 KB.
    `xml`: metadata about the dataset, size 16 KB.

    b.     `dbf`:size 6.4 MB.
    `shp`: size 837.5 MB.
    `prj`: size 165 bytes.
    `shx`: size 265 KB.
    `xml`: size 16 KB.
    
2. 
```{python}
import geopandas as gpd 
```

```{python}
filepath = "/Users/nasser.alshaya/Desktop/problem-set-4-nasser-daniel/gz_2010_us_860_00_500k"

path_shp = os.path.join(filepath,
 "gz_2010_us_860_00_500k.shp")
df_zip = gpd.read_file(path_shp)
```

```{python}
texas_zip_codes = df_zip[df_zip["ZCTA5"].str.startswith(
    ("70","71","72","73","74","75","76","77","78","79"))]
```

```{python}
'''
texas_zip_codes[texas_zip_codes["ZCTA5"].isin(df_2016["ZIP_CD"])]
'''
```

```{python}
import matplotlib.pyplot as plt
texas_zip_codes.plot(facecolor="none", linewidth=0.2)
plt.axis("off")
plt.show()
```


## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
