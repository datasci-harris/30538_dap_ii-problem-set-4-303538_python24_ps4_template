---
title: "Problem Set 4 - Hospital Data"
author: "Mario Venegas and Neil Stein"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

```{python}
import pandas as pd

pos2016 = pd.read_csv('pos2016.csv')
print(pos2016.head())
```

1. 

```{python}
print("Variable names:", pos2016.columns.tolist())
```

2. 
    a.
```{python} 
unique_values = pos2016['PRVDR_CTGRY_CD'].unique()
print("Unique values for PRVDR_CTGRY_CD:", unique_values)

unique_values = pos2016['PRVDR_CTGRY_SBTYP_CD'].unique()
print("Unique values for PRVDR_CTGRY_SBTYP_CD:", unique_values)
```

```{python} 
pos2016 = pos2016[pos2016['PRVDR_CTGRY_CD'] == 1]
pos2016 = pos2016[pos2016['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2016.head())
```

```{python}
num_observations = pos2016.shape[0]
print("Number of observations in pos2016:", num_observations)
```

    b. By comparing numebr of observations for 2016 to the ones for 2017, 2018, and 2019, one can observe that the number is low. It could be possible that this way of identifying providers started to be used in 2017. 

3. 

2017
```{python}
pos2017 = pd.read_csv('pos2017.csv')
print(pos2017.head())
```

```{python} 
pos2017 = pos2017[pos2017['PRVDR_CTGRY_CD'] == 1]
pos2017 = pos2017[pos2017['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2017.head())
```

```{python}
num_observations = pos2017.shape[0]
print("Number of observations in pos2017:", num_observations)
```

2018
```{python}
pos2018 = pd.read_csv('pos2018.csv', encoding='ISO-8859-1')
```

```{python} 
pos2018 = pos2018[pos2018['PRVDR_CTGRY_CD'] == 1]
pos2018 = pos2018[pos2018['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2018.head())
```

```{python}
num_observations = pos2018.shape[0]
print("Number of observations in pos2018:", num_observations)
```

2019
```{python}
pos2019 = pd.read_csv('pos2019.csv', encoding='ISO-8859-1')
```

```{python} 
pos2019 = pos2019[pos2019['PRVDR_CTGRY_CD'] == 1]
pos2019 = pos2019[pos2019['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2017.head())
```

```{python}
num_observations = pos2019.shape[0]
print("Number of observations in pos2019:", num_observations)
```

Append
```{python}
pos2017['year'] = 2017
pos2018['year'] = 2018
pos2019['year'] = 2019

print(pos2017.head)
```

```{python}
combined_df = pd.concat([pos2017, pos2018, pos2019], axis=0, ignore_index=True)

print(combined_df.columns)
```

```{python}
import altair as alt
```

```{python}
observations_per_year = combined_df.groupby('year').size().reset_index(name='count')

chart = alt.Chart(observations_per_year).mark_bar().encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('count:Q', title='Number of Observations'),
    tooltip=['year:O', 'count:Q']
).properties(
    title='Number of Observations Per Year'
)

chart
```

4. 
    a.
```{python}
unique_prvdr_counts = combined_df.groupby('year')['PRVDR_NUM'].nunique().reset_index()

unique_prvdr_counts.columns = ['year', 'unique_prvdr_count']
```

```{python}
chart = alt.Chart(unique_prvdr_counts).mark_bar().encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('unique_prvdr_count:Q', title='Unique Number of Hospitals'),
    tooltip=['year:O', 'unique_prvdr_count:Q']
).properties(
    title='Unique Number of Hospitals Per Year'
)
chart
```

    b.In 2019, the way of identifying hospitals changed. So this variable was not reported for any hospital this year. 

## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}
import pandas as pd

# finding suspect closures - joining data

all_years_df = pd.merge(pos2016, combined_df, on= "FAC_NAME")
closures_df = all_years_df.groupby(["FAC_NAME", "ZIP_CD_y"])["year"].max().reset_index()


# isolating closures
target_year = 2019
closed_early = closures_df[closures_df["year"] < target_year]
print(f"There are {len(closed_early)} hospitals fitting this description")
```

2. 

```{python}
import pandas as pd

# sorting and finding the top 10

closed_early_sorted = closed_early.sort_values(by= "FAC_NAME")
print(closed_early_sorted[["FAC_NAME","year"]].head(10))
```

3. 
    a.
```{python}
import pandas as pd

# function to comb through and check total hospitals by zip across years
def merger_checker(df):
    grouped_df = df.groupby(["ZIP_CD", "year", "FAC_NAME", "PRVDR_NUM"]).size().reset_index(name= "count") 
    grouped_df = grouped_df.sort_values(["ZIP_CD", "year"])
    grouped_df["merger_suspect"] = grouped_df.groupby("ZIP_CD")["count"].diff() !=0
    grouped_df["merger_suspect"] = grouped_df["merger_suspect"].fillna(False)
    return grouped_df

merger_flagged_df = merger_checker(combined_df)
'''
print(merger_flagged_df.head(10))
'''

# checked the function via print, looks good! now we filter
merge_filter = merger_flagged_df[merger_flagged_df["merger_suspect"] == True]

```

    b.
 ```{python}
import pandas as pd

# handling empty/NA values
merge_filter["PRVDR_NUM"] = merge_filter["PRVDR_NUM"].fillna("terminated")
print(f"There are {sum(merge_filter["PRVDR_NUM"] == "terminated")} hospitals fitting this description")

# correcting to subset out the terminated codes
terminated_cleaned = merge_filter[merge_filter["PRVDR_NUM"] != "terminated"]
print(f"There are {len(terminated_cleaned)} hospitals that are left fitting this description")
 ```

    c.
```{python}
import pandas as pd

# sorting by name
terminated_cleaned = terminated_cleaned.sort_values(by= "FAC_NAME")
print(terminated_cleaned["FAC_NAME"].head(10))
```


## Download Census zip code shapefile (10 pt) 

1. 
    a.
    b. 
2. 

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
