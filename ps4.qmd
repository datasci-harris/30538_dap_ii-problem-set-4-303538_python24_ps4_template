---
title: "PS4: Spatial"
author: "Alison Filbey and Claire Conzelmann " 
date: "today"
format: pdf
geometry: margin=1in
fontsize: 12pt
execute:
  eval: false
  echo: true
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.
• Partner 1 (name and cnet ID): Alison Filbey afilbey
• Partner 2 (name and cnet ID): Claire Conzelmann cconzelmann
3. Partner 1 will accept the ps4 and then share the link it creates with their partner.
You can only share it with one partner so you will not be able to change it after your
partner has accepted.
4. “This submission is our work alone and complies with the 30538 integrity policy.” Add your initials to indicate your agreement: **CC** **AF**
5. “I have uploaded the names of anyone else other than my partner and I worked with
on the problem set here” (1 point)
6. Late coins used this pset: **0** Late coins left after submission: **4**
7. Knit your ps4.qmd to an PDF file to make ps4.pdf,
• The PDF should not be more than 25 pages. Use head() and re-size figures when
appropriate.
8. (Partner 1): push ps4.qmd and ps4.pdf to your github repo.
9. (Partner 1): submit ps4.pdf via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

## Download and explore the Provider of Services (POS) file (10 pts)

```{python}
#libraries
import pandas as pd
import altair as alt
alt.data_transformers.enable("vegafusion")
import numpy as np
```

1. (Partner 1) This is a fairly large dataset and we won’t be using most of the variables. Read through the rest of the problem set and look through the data dictionary to identify which variables you will need to complete the exercise, and use the tool on data.cms.gov into restrict to those variables (“Manage Columns”) before exporting(“Export”). Download this for 2016 and call it pos2016.csv. What are the variables you pulled?

The variables I pulled as potentially useful are as follows:

| Variable Name                          | Description                             |
| -------------------------------------- | ----------------------------------------|
| PRVDR_CTGRY_SBTYP_CD                   | Provider category and subcategory       |
| PRVDR_CTGRY_CD                         | Provider category                       |
| FAC_NAME                               | Facility Name                           | 
| PRVDR_NUM                              | CMS certification number                |
| STATE_CD                               | State                                   | 
| PGM-TRMTN-CD                           | Termination Code                        | 
| TRMNTN-EXPRTN-DT                       | Termination Date                        | 
| ZIP_CD                                 | Zip code                                | 


2. (Partner 1) Import your pos2016.csv file. We want to focus on short-term hospitals. These are identified as facilities with provider type code 01 and subtype code 01. Subset your data to these facilities. How many hospitals are reported in this data? Does this number make sense? Cross-reference with other sources and cite the number you compared it to. If it differs, why do you think it could differ?
```{python}
#Reading in 2016 data and filtering for short-term hospitals
pos2016 = pd.read_csv('Data/POS_File_Hospital_Non_Hospital_Facilities_Q4_2016.csv')
pos2016 = pos2016[(pos2016["PRVDR_CTGRY_SBTYP_CD"] == 1) & (pos2016["PRVDR_CTGRY_CD"] == 1)]
print(len(pos2016))

```

The dataset has 7,245 observations. Upon first glance, this number seems reasonable. This is seemingly more than the number of registered hospitals in the [2018 American Hospital Association Statistics fact sheet](https://www.aha.org/system/files/2018-02/2018-aha-hospital-fast-facts.pdf), which uses data from the 2016 AHA Annual Survey. This discrepancy could come from differences in the definition of a hospital, which according to AHA must meet"AHA's criteria for registration as a hospital facility". This may differ from the CMS criteria for defining a short-term hospital. 

3. (Partner 1) Repeat the previous 3 steps with 2017Q4, 2018Q4, and 2019Q4 and then append them together. Plot the number of observations in your dataset by year.
```{python}
#Reading the datasets and creating a columns called year
pos2016['year']=2016
#2017
pos2017 = pd.read_csv('Data/POS_File_Hospital_Non_Hospital_Facilities_Q4_2017.csv')
pos2017 = pos2017[(pos2017["PRVDR_CTGRY_SBTYP_CD"] == 1) & (pos2017["PRVDR_CTGRY_CD"] == 1)]
pos2017['year']=2017
#2018
pos2018 = pd.read_csv('Data/POS_File_Hospital_Non_Hospital_Facilities_Q4_2018.csv', encoding='ISO-8859-1')
pos2018 = pos2018[(pos2018["PRVDR_CTGRY_SBTYP_CD"] == 1) & (pos2018["PRVDR_CTGRY_CD"] == 1)]
pos2018['year']=2018
#2019
pos2019 = pd.read_csv('Data/POS_File_Hospital_Non_Hospital_Facilities_Q4_2019.csv', encoding='ISO-8859-1')
pos2019 = pos2019[(pos2019["PRVDR_CTGRY_SBTYP_CD"] == 1) & (pos2019["PRVDR_CTGRY_CD"] == 1)]
pos2019['year']=2019

#Appending the datasets
pos = pd.concat([pos2016, pos2017, pos2018, pos2019])

#Creating a plot of number of observations per year
plot1 = alt.Chart(pos, width=300, height=300, title='Number of Hospitals Per Year, 2016-2019').mark_circle().encode(
    alt.X('year:O', axis=alt.Axis(title= 'Year')),
    alt.Y('count()', axis=alt.Axis(title= 'Number of Hospitals'))
)

plot1
```


4. (Partner 1) Each hospital is identified by its CMS certification number. Plot the number of unique hospitals in your dataset per year. Compare this to your plot in the previous step. What does this tell you about the structure of the data?

```{python}
#Creating a plot of number of observations per year
plot2 = alt.Chart(pos, width=300, height=300, title='Number of Unique Hospitals Per Year, 2016-2019').mark_circle().transform_aggregate(
    groupby=['PRVDR_NUM', 'year'],
    unique_CMS='count():Q').encode(
    alt.X('year:O', axis=alt.Axis(title= 'Year')),
    alt.Y('count(unique_CMS):Q', axis=alt.Axis(title= 'Number of Hospitals'))
)

plot2
```

Comparing plot1 and plot2, it looks as if even after we restrict our number of hospitals to hose with unique CMS certification numbers, the the number of hospitals per year remains constant. This tells us that the CMS certification number is a unique identifier of our data as every hospital has exactly one unique CMS number.

## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}
#create indicator for hospitals acitve in 2016
pos["active_2016"] = np.where(
  (pos["year"]==2016) & (pos["PGM_TRMNTN_CD"]==0), 1, 0)
pos["active_2016"] = pos.groupby("PRVDR_NUM")["active_2016"].transform("max")

#find max year each hospital appears as active in dataset
pos["max_year"] = pos.groupby("PRVDR_NUM")["year"].transform("max")

#create indicator if hospital is ever not active
pos["ever_inactive"] = np.where(
  pos.groupby("PRVDR_NUM")["PGM_TRMNTN_CD"].transform("max")!=0, 1, 0)

#keep hospitals active in 2016 and no longer active
inactive_hospitals = pos.loc[(pos["active_2016"]==1) & (pos["ever_inactive"]==1)]

#groupby hospital and status 
grouped = inactive_hospitals.groupby(
  ["PRVDR_NUM", "PGM_TRMNTN_CD"])["year"].min().reset_index()

#drop active rows 
grouped = grouped.loc[grouped["PGM_TRMNTN_CD"] != 0]

#rename year
grouped = grouped.rename({"year":"inactive_year"}, axis=1).drop(
  "PGM_TRMNTN_CD", axis=1)

#merge inactive year into inactive hospitals df
inactive_hospitals= pd.merge(inactive_hospitals, grouped, on="PRVDR_NUM", how="left")

#keep requested variables
inactive_hospitals = inactive_hospitals[[
  "FAC_NAME", "ZIP_CD", "inactive_year", "PRVDR_NUM", "year"]]

#count number of hospitals that went inactive
len(inactive_hospitals["PRVDR_NUM"].unique())
```

There are 174 hospitals that became inactive at some point between 2016 and 2019.

2. 

```{python}
#drop duplicates
inactive_hospitals_nodup = inactive_hospitals.drop_duplicates(subset="PRVDR_NUM")

#sort by facility name and print first 10 rows
inactive_hospitals_nodup[["FAC_NAME", "inactive_year"]].sort_values(
  "FAC_NAME").head(10)
```

3. 

```{python}
#calculate number of active hospitals in each zipcode by year
active_count = pos[pos["PGM_TRMNTN_CD"] == 0]
active_count = active_count.groupby(["ZIP_CD", "year"]).size().reset_index(
  name="n_fac_zip")

#merge the counts back into the original df and the inactive df
pos = pd.merge(pos, active_count, on=["ZIP_CD", "year"], how="left")
inactive_hospitals = pd.merge(
  inactive_hospitals, active_count, on=["ZIP_CD", "year"], how="left")

#replace missing n active hospitals with zero
pos["n_fac_zip"].fillna(0, inplace=True)
inactive_hospitals["n_fac_zip"].fillna(0, inplace=True)

#create lag for number of hospitals in each zipcode
#I googled how to create a lag of a variable and followed this example
#https://discuss.datasciencedojo.com/t/how-to-create-lags-and-leads-of-a-column/1023
inactive_hospitals = inactive_hospitals.sort_values(by=["PRVDR_NUM", "year"])
inactive_hospitals["lag_n_fac_zip"] = inactive_hospitals.groupby(
  "PRVDR_NUM")["n_fac_zip"].shift(1)

#keep hospitals in zipcodes that saw decrease in number of hospitals after closure
inactive_hospitals = inactive_hospitals.loc[
  (inactive_hospitals["year"]==inactive_hospitals["inactive_year"]) &
  (inactive_hospitals["lag_n_fac_zip"] > inactive_hospitals["n_fac_zip"])]
```

**i** Before this correction, there were 174 hospitals that had a suspected closure between 2016 and 2019. After this correction, there are 167 hospitals that had a suspected closure, meaning there are 7 suspected mergers/aquisitions. 

**ii** 
```{python}
#sort by facility name and print first 10 rows
inactive_hospitals[["FAC_NAME", "inactive_year"]].sort_values(
  "FAC_NAME").head(10)
```

## Download Census zip code shapefile (10 pt) 

1. 
2. 

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
5. 

## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 