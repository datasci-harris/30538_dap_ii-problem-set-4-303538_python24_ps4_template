---
title: "PS4 v1.1: Spatial"
author: "Boya Lin and Zidan Kong"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Boya Lin, boya1
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*BL\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\*\* Late coins left after submission: \*\*\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

```{python}
import pandas as pd
import altair as alt
alt.renderers.enable("png")
```

## Download and explore the Provider of Services (POS) file (10 pts)

1. 

Vairables we pulled in pos2016.csv include PRVDR_CTGRY_SBTYP_CD, PRVDR_CTGRY_CD, CITY_NAME, FAC_NAME (Facility Name), ORGNL_PRTCPTN_DT, PRVDR_NUM (CMS Certification Number), PGM_TRMNTN_CD (Termination Code), TRMNTN_EXPRTN_DT, and ZIP_CD.

2. 
    a.

```{python}
pos_2016 = pd.read_csv('pos2016.csv')
short_term_hospitals_2016 = pos_2016[(pos_2016['PRVDR_CTGRY_CD'] == 1) & (
    pos_2016['PRVDR_CTGRY_SBTYP_CD'] == 1)].copy()
# ChatGDP reference for SettingWithCopyWarning, the usage of .copy

count_2016 = short_term_hospitals_2016.shape[0]
print(f'Number of short-term hospitals in 2016: {count_2016}')
```

This number appears slightly high, since the American Hospital Association (AHA) reported a total of 6,120 hospitals in the U.S. in 2024. In consideration of the growing hospital industry, we would generally expect the number of hospitals in 2016 to be lower than that in 2024.

Data Source (AHA): https://www.aha.org/statistics/fast-facts-us-hospitals 

    b.

Unlike 7,245 hospitals reported in our data, the American Hospital Association (AHA) reports a total number of 5,534 U.S. hospitals for 2016. The difference may arise from factors such as hospitals name changes, acquisitions, or the presence of branches or devisions, which can lead to multiple entries for the same facility in our data. As a result, when using .nunique(), these hospitals/facilitates might be counted separately, leading to an inflated total. 

Data Source (AHA): chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.aha.org/
system/files/2018-01/Fast%20Facts%202018%20pie%20charts.pdf 

3. 

```{python}
pos_2017 = pd.read_csv('pos2017.csv')
short_term_hospitals_2017 = pos_2017[(pos_2017['PRVDR_CTGRY_CD'] == 1) & (
    pos_2017['PRVDR_CTGRY_SBTYP_CD'] == 1)].copy()

pos_2018 = pd.read_csv('pos2018.csv', encoding='ISO-8859-1')
short_term_hospitals_2018 = pos_2018[(pos_2018['PRVDR_CTGRY_CD'] == 1) & (
    pos_2018['PRVDR_CTGRY_SBTYP_CD'] == 1)].copy()

pos_2019 = pd.read_csv('pos2019.csv', encoding='ISO-8859-1')
short_term_hospitals_2019 = pos_2019[(pos_2019['PRVDR_CTGRY_CD'] == 1) & (
    pos_2019['PRVDR_CTGRY_SBTYP_CD'] == 1)].copy()
# ChatGDP reference for "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 119424", solution: add encoding='ISO-8859-1'

short_term_hospitals_2016.loc[:, 'Year'] = 2016
short_term_hospitals_2017.loc[:, 'Year'] = 2017
short_term_hospitals_2018.loc[:, 'Year'] = 2018
short_term_hospitals_2019.loc[:, 'Year'] = 2019
all_hospitals = pd.concat([short_term_hospitals_2016,
                           short_term_hospitals_2017,
                           short_term_hospitals_2018,
                           short_term_hospitals_2019], ignore_index=True)
# Pandas official website reference for pd.concat: https://pandas.pydata.org/docs/reference/api/pandas.concat.html

hospitals_by_year = all_hospitals.groupby(
    'Year').size().reset_index(name='Number of Hospitals')

bar_chart = alt.Chart(hospitals_by_year).mark_bar().encode(
    x='Year:O', y='Number of Hospitals:Q', color='Year:O'
).properties(
    title='Number of Short-Term Hospitals by Year', width=360, height=260
)

text_labels = bar_chart.mark_text(
    align='center', baseline='bottom', dy=-5).encode(text='Number of Hospitals:Q')
hospital_chart = bar_chart + text_labels
hospital_chart
# ChatGDP reference for adding exact values of data above bar chart
```

4. 
    a.

```{python}
unique_hospitals_by_year = all_hospitals.groupby(
    'Year')['PRVDR_NUM'].nunique().reset_index(name='Number of Unique Hospitals')

bar_chart = alt.Chart(unique_hospitals_by_year).mark_bar().encode(
    x='Year:O', y='Number of Unique Hospitals:Q', color='Year:O'
).properties(
    title='Number of Unique Short-Term Hospitals by Year',
    width=360, height=260
)

text_labels = bar_chart.mark_text(
    align='center', baseline='bottom', dy=-5).encode(text='Number of Unique Hospitals:Q')
unique_hospitals_chart = bar_chart + text_labels
unique_hospitals_chart
```

    b. 

Both plots present the same counts of observations for each year. It suggests that each observation in our dataset corresponds to a unique hospital (identified by its CMS certification number). In other words, there are no duplicate reocrds for nay hospital in any year. 

## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}
# Filter active hospitals in 2016
active_2016 = pos_2016[pos_2016['PGM_TRMNTN_CD'] == 0]
active_2016 = active_2016[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']]
```

```{python}
print("Columns in active_2016:", active_2016.columns)
print("Columns in pos_2017 before merging:", pos_2017.columns)
# Merge with 2017 data
merged_2017 = active_2016.merge(
    pos_2017[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD', 'PGM_TRMNTN_CD']],
    on='PRVDR_NUM',
    how='left',
    indicator="_merge_2017",
    suffixes=('', '_2017')
)
#merged_2017 = active_2016.merge(pos_2017[['PRVDR_NUM','FAC_NAME','ZIP_CD','PGM_TRMNTN_CD']], on='PRVDR_NUM', how='left', indicator="_merge_2017")
print("Columns in pos_2017 after merging:", merged_2017.columns)
closed_2017 = merged_2017[(merged_2017['_merge_2017'] == 'left_only') | (merged_2017['PGM_TRMNTN_CD'].fillna(1) != 0)]

# Add the suspected closure year
closed_2017.loc[:, 'Year of Suspected Closure'] = 2017
print("Columns in closed_2017:", closed_2017.columns)
closed_2017 = closed_2017[['FAC_NAME_2017', 'ZIP_CD_2017', 'Year of Suspected Closure','PRVDR_NUM']]
```

```{python}
merged_2018 = closed_2017.merge(
    pos_2018[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD', 'PGM_TRMNTN_CD']],
    on='PRVDR_NUM',
    how='left',
    indicator="_merge_2018",
    suffixes=('', '_2018')
)
#merged_2017 = active_2016.merge(pos_2017[['PRVDR_NUM','FAC_NAME','ZIP_CD','PGM_TRMNTN_CD']], on='PRVDR_NUM', how='left', indicator="_merge_2017")
print("Columns in pos_2017 after merging:", merged_2018.columns)
closed_2018 = merged_2018[(merged_2018['_merge_2018'] == 'left_only') | (merged_2018['PGM_TRMNTN_CD'].fillna(1) != 0)]

# Add the suspected closure year
closed_2018.loc[:, 'Year of Suspected Closure'] = 2018
print("Columns in closed_2017:", closed_2018.columns)
closed_2018 = closed_2018[['FAC_NAME', 'ZIP_CD', 'Year of Suspected Closure','PRVDR_NUM']]
```


```{python}
merged_2019 = closed_2018.merge(
    pos_2019[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD', 'PGM_TRMNTN_CD']],
    on='PRVDR_NUM',
    how='left',
    indicator="_merge_2019",
    suffixes=('', '_2019')
)
#merged_2017 = active_2016.merge(pos_2017[['PRVDR_NUM','FAC_NAME','ZIP_CD','PGM_TRMNTN_CD']], on='PRVDR_NUM', how='left', indicator="_merge_2017")
print("Columns in pos_2017 after merging:", merged_2018.columns)
closed_2019 = merged_2019[(merged_2019['_merge_2019'] == 'left_only') | (merged_2019['PGM_TRMNTN_CD'].fillna(1) != 0)]

# Add the suspected closure year
closed_2019.loc[:, 'Year of Suspected Closure'] = 2018
print("Columns in closed_2017:", closed_2019.columns)
closed_2019 = closed_2019[['FAC_NAME_2019', 'ZIP_CD_2019', 'Year of Suspected Closure','PRVDR_NUM']]
```

```{python}
closed_2017 = closed_2017.rename(columns={'ZIP_CD_2017': 'ZIP_CD'})
closed_2019 = closed_2019.rename(columns={'ZIP_CD_2019': 'ZIP_CD'})

full_closures = pd.concat([closed_2017, closed_2018, closed_2019], ignore_index=True)
full_closures= full_closures.drop_duplicates(subset=['FAC_NAME', 'ZIP_CD'], keep='first')

number_of_hospital=len(full_closures)
number_of_hospital
```

Referencing Chagpt with suffixes, asking how do i solve the problem when oit creates x and y column for coulumn appears in both dataset.


2. 
```{python}
sorted_closed= full_closures.sort_values(by='FAC_NAME')
# Select only the columns for facility name and year of suspected closure
first_10 = sorted_closed[['FAC_NAME', 'Year of Suspected Closure']].head(10)
# Display the first 10 rows
print(first_10)
```

3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 

1. 
    a.
    b. 
2. 

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
